# code/cwdm-main/scripts/sample.py

import argparse
import nibabel as nib
import numpy as np
import os
import pathlib
import random
import sys
import torch as th
from tqdm import tqdm

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.getcwd())

from guided_diffusion import dist_util, logger
from guided_diffusion.bratsloader import BRATSVolumes
# â˜…â˜… ãƒ¢ãƒ‡ãƒ«ã‚’å¼•æ•°ã‹ã‚‰ä½œæˆã™ã‚‹ãŸã‚ã®æ­£ã—ã„é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â˜…â˜…
from guided_diffusion.script_util import (
    create_model,
    add_dict_to_argparser,
)
from DWT_IDWT.DWT_IDWT_layer import IDWT_3D, DWT_3D

@th.no_grad()
def ode_sampler(model, cond, shape, device, steps=100):
    """
    Flow-Matchingãƒ¢ãƒ‡ãƒ«ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚ªã‚¤ãƒ©ãƒ¼æ³•ODEã‚½ãƒ«ãƒãƒ¼
    """
    x_t = th.randn(shape, device=device)
    time_steps = th.linspace(0, 1, steps + 1, device=device)

    for i in tqdm(range(steps), desc="ODE Sampling", leave=False):
        t_start, t_end = time_steps[i], time_steps[i+1]
        dt = t_end - t_start
        model_input = th.cat([x_t, cond], dim=1)
        # ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦æ‹¡å¼µã™ã‚‹
        v_t = model(model_input, t_start.expand(x_t.size(0)))
        x_t = x_t + v_t * dt
    
    return x_t

def main():
    args = create_argparser().parse_args()
    dist_util.setup_dist(devices=[str(d) for d in args.devices])
    logger.configure(dir=args.output_dir)

    logger.log("Loading checkpoint and creating model...")
    checkpoint = dist_util.load_state_dict(args.model_path, map_location="cpu")
    
    # --- ğŸ’¡ã€ã“ã“ãŒæ°¸ç¶šçš„ãªæœ€çµ‚ä¿®æ­£ã§ã™ã€‘ğŸ’¡ ---

    # 1. create_modelãŒå—ã‘ä»˜ã‘ã‚‹ã€Œæ­£ã—ã„å¼•æ•°ã€ã®ãƒªã‚¹ãƒˆã‚’å®šç¾©ã—ã¾ã™ã€‚
    #    (ã“ã‚Œã¯ train.py ã§ä½¿ã£ãŸã‚‚ã®ã¨åŒã˜ãƒªã‚¹ãƒˆã§ã™)
    known_model_args = [
        'image_size', 'in_channels', 'num_channels', 'out_channels', 
        'num_res_blocks', 'attention_resolutions', 'dropout', 'channel_mult', 
        'use_checkpoint', 'use_fp16', 'num_heads', 'num_head_channels', 
        'use_scale_shift_norm', 'resblock_updown', 'use_new_attention_order', 
        'dims', 'num_groups', 'renormalize', 'additive_skips', 'use_freq', 
        'bottleneck_attention', 'resample_2d'
    ]

    # 2. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰èª­ã¿è¾¼ã‚“ã å¼•æ•°ã®ã†ã¡ã€ã€Œæ­£ã—ã„å¼•æ•°ã€ã ã‘ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    all_loaded_args = checkpoint['model_args']
    clean_model_args = {
        key: all_loaded_args[key] 
        for key in known_model_args 
        if key in all_loaded_args
    }
    
    # 3. æŠ½å‡ºã—ãŸã‚¯ãƒªãƒ¼ãƒ³ãªå¼•æ•°ã ã‘ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚
    model = create_model(**clean_model_args)
    
    # --- âœ… ã“ã‚Œã§ã€ã“ã®ç¨®ã®ã‚¨ãƒ©ãƒ¼ã¯äºŒåº¦ã¨ç™ºç”Ÿã—ã¾ã›ã‚“ âœ… ---
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(dist_util.dev())
    model.eval()
    logger.log("Model created and loaded successfully.")

    logger.log("Creating data loader...")
    ds = BRATSVolumes(args.data_dir, mode='eval')
    datal = th.utils.data.DataLoader(ds, batch_size=args.batch_size, shuffle=False)

    dwt = DWT_3D("haar").to(dist_util.dev())
    idwt = IDWT_3D("haar").to(dist_util.dev())

    th.manual_seed(args.seed)
    np.random.seed(args.seed)
    random.seed(args.seed)

    logger.log(f"Starting sampling with {args.ode_steps} steps...") # ãƒ­ã‚°ã«ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¡¨ç¤º
    for i, (target_batch, cond_batch) in enumerate(tqdm(datal, desc="Processing Subjects")):
        cond_image = cond_batch['cond_1'].to(dist_util.dev())

        with th.no_grad():
            cond_wav_components = dwt(cond_image)
            cond_wav = th.cat(cond_wav_components, dim=1)
        
        out_channels = checkpoint['model_args'].get('out_channels', 8)
        sample_shape = (args.batch_size, out_channels, 112, 112, 80)
        
        # ğŸ’¡ ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸ100ã®ä»£ã‚ã‚Šã«ã€å¼•æ•°ã§å—ã‘å–ã£ãŸã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’ä½¿ã„ã¾ã™
        sample_wav = ode_sampler(model, cond_wav, sample_shape, dist_util.dev(), steps=args.ode_steps)


        # ... (ä»¥é™ã®ç”»åƒä¿å­˜å‡¦ç†ã¯å¤‰æ›´ãªã—)
        B, _, D, H, W = sample_wav.shape
        sample_idwt = idwt(
            sample_wav[:, 0:1, ...], sample_wav[:, 1:2, ...], sample_wav[:, 2:3, ...], sample_wav[:, 3:4, ...],
            sample_wav[:, 4:5, ...], sample_wav[:, 5:6, ...], sample_wav[:, 6:7, ...], sample_wav[:, 7:8, ...]
        )
        sample_idwt[sample_idwt < 0] = 0
        sample_idwt[sample_idwt > 1] = 1
        sample_idwt[cond_image == 0] = 0
        sample_idwt = sample_idwt.squeeze(dim=1)[:, :, :, :155]
        
        subj_name = f"Sample_{i:04d}"
        subj_dir = os.path.join(args.output_dir, subj_name)
        pathlib.Path(subj_dir).mkdir(parents=True, exist_ok=True)
        
        output_name = os.path.join(subj_dir, 'sample_t1c_from_t1n.nii.gz')
        img = nib.Nifti1Image(sample_idwt.detach().cpu().numpy()[0], np.eye(4))
        nib.save(img=img, filename=output_name)

    logger.log(f"Sampling complete. Results saved to {args.output_dir}")

def create_argparser():
    """
    ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«å¿…è¦ãªå¼•æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚
    """
    defaults = dict(
        seed=42,
        data_dir="",
        model_path="",
        batch_size=1,
        output_dir="./results_flow_matching",
        devices=[0],
        # ğŸ’¡ã€ã“ã‚Œã‚’è¿½åŠ ã€‘: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’æŒ‡å®šã™ã‚‹å¼•æ•°ã‚’è¿½åŠ ã—ã¾ã™
        ode_steps=100, 
    )
    parser = argparse.ArgumentParser()
    add_dict_to_argparser(parser, defaults)
    return parser

if __name__ == "__main__":
    main()