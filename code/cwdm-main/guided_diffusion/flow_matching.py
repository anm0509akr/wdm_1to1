# code/cwdm-main/guided_diffusion/flow_matching.py

import torch
import torch.nn as nn
from tqdm import tqdm

class FlowMatching(nn.Module):
    def __init__(self, num_timesteps=1000):
        super().__init__()
        self.num_timesteps = num_timesteps

    def get_vector_field(self, x_0, x_1):
        """
        æœ€ã‚‚å˜ç´”ãªç·šå½¢è£œé–“ã®å ´åˆã€ãƒ™ã‚¯ãƒˆãƒ«å ´ã¯ x_1 - x_0 ã¨ãªã‚Šã¾ã™ã€‚
        """
        return x_1 - x_0

    def training_losses(self, model, x_0, x_1, t, model_kwargs=None):
        """
        Flow-Matchingã®æå¤±ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
        """
        if model_kwargs is None:
            model_kwargs = {}

        cond_wavelet = model_kwargs.get("cond_wav")
        if cond_wavelet is None:
            raise ValueError("æ¡ä»¶Wavelet 'cond_wav' ãŒ model_kwargs ã®ä¸­ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        t_reshaped = t.view(-1, 1, 1, 1, 1)
        x_t = (1 - t_reshaped) * x_0 + t_reshaped * x_1

        true_vector_field = self.get_vector_field(x_0, x_1)
        model_input = torch.cat([x_t, cond_wavelet], dim=1)
        predicted_vector_field = model(model_input, t)
        
        # --- ğŸ’¡ã€ã“ã“ãŒæœ€å¾Œã®ä¿®æ­£ç‚¹ã§ã™ã€‘ğŸ’¡ ---

        # 1. reduction='none' ã‚’æŒ‡å®šã—ã¦ã€æå¤±ã‚’ãƒ”ã‚¯ã‚»ãƒ«ã”ã¨ã€ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«è¨ˆç®—ã—ã¾ã™ã€‚
        #    ã“ã‚Œã«ã‚ˆã‚Šã€loss_per_pixelã®å½¢çŠ¶ã¯ (B, C, D, H, W) ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
        loss_per_pixel = nn.functional.mse_loss(
            predicted_vector_field, true_vector_field, reduction='none'
        )

        # 2. ãƒãƒƒãƒæ¬¡å…ƒ(dim=0)ä»¥å¤–ã®å…¨ã¦ã®æ¬¡å…ƒã§å¹³å‡ã‚’å–ã‚Šã¾ã™ã€‚
        #    ã“ã‚Œã«ã‚ˆã‚Šã€æœ€çµ‚çš„ãªlossã®å½¢çŠ¶ã¯ (B,) ã¨ãªã‚Šã€
        #    å„ã‚µãƒ³ãƒ—ãƒ«ã®å¹³å‡æå¤±ãŒæ ¼ç´ã•ã‚ŒãŸ1æ¬¡å…ƒã®é…åˆ—ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚
        loss = loss_per_pixel.mean(dim=list(range(1, len(loss_per_pixel.shape))))
        
        # --- âœ… ä¿®æ­£å®Œäº† âœ… ---

        return {"loss": loss}


    @torch.no_grad()
    def sample_loop(self, model, shape, cond, device='cuda', ode_steps=100):
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™ã€‚
        """
        x_t = torch.randn(shape, device=device)
        time_steps = torch.linspace(0, 1, ode_steps + 1, device=device)
        
        for i in tqdm(range(ode_steps), desc="Sampling with Flow-Matching", leave=False):
            t_start, t_end = time_steps[i], time_steps[i+1]
            dt = t_end - t_start
            
            model_input = torch.cat([x_t, cond], dim=1)
            t_batch = t_start.expand(x_t.size(0))
            v_t = model(model_input, t_batch)
            
            x_t = x_t + v_t * dt

        return x_t